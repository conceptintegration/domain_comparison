#!/bin/python
# -*- coding: utf-8 -*-

__author__      = 'Roy Gardner'
__copyright__   = 'Copyright 2025, Roy Gardner and Sally Gardner'


"""
Generates ontology data model files in the ../model folder:

CCP-FACET
IDEA-GLO

Each ontology has the following files:

<ontology_label>_encoded_topics.json
    - List of CCP-FACET ontology topic IDs
<ontology_label>_topic_encodings.json
    - List of topic text encodings generated by Google's multilingual Universal Sentence Encoder version 3
    - Topic text is a concatenation of the topic text and topic description text fields
<ontology_label>_topic_segment_matrix.json
    - Semantic similarity matrix with topics in rows and segments in columns
<ontology_label>_topics_dict.json
    - A dictionary containing topic data
    - The key is a topic ID
    - Value is a dictionary topic key, label, and description values. 
    - Also concatenation of the topic text and topic description text fields used to generate encoding vector


"""

from packages import *
from nlp import *
from semantic import *


def process(config):
    # Encoder
    encoder = hub.load(config['nlp']['encoder_path'] + config['nlp']['encoder'])
    model_path = config['ontologies']['model_path']

    print('Loading segment encodings…')
    with open(config['constitutions']['model_path'] + 'segment_encodings.json', 'r', encoding='utf-8') as f:
        segment_encodings = json.load(f)
        f.close() 
    
    ontologies_dict = {}
    reference_ont_label = config['ontologies']['reference']

    _, dirs,_ = next(os.walk(config['ontologies']['path']))
    # Using the directory name as the ontology label
    dirs = sorted(dirs)
    for ont_label in dirs:
        ont_path = config['ontologies']['path'] + os.sep + ont_label + os.sep
        ontology_file = ont_path + 'ontology.csv'
        metadata_file = ont_path + 'metadata.csv'
        # organization	name	label	description	citation	version
        with open(metadata_file, encoding='utf-8', errors='replace') as f:
            reader = csv.reader(f)
            header = list(next(reader))
            data = [row for row in reader]
            f.close()

            ontologies_dict[ont_label] = {}
            if ont_label == reference_ont_label:
                ontologies_dict[ont_label]['reference'] = True
            else:
                ontologies_dict[ont_label]['reference'] = False

            for field in header:
                ontologies_dict[ont_label][field] = data[0][header.index(field)]

        with open(ontology_file, encoding='utf-8', errors='replace') as f:
            reader = csv.reader(f)
            # Get the header row
            ont_header = next(reader)
            ont_data = [row for row in reader]
            f.close

        topics_dict = {}
        for i,row in enumerate(ont_data):
            topic_id = str(row[ont_header.index('Key')]).strip()
            if len(topic_id) == 0:
                continue
            topics_dict[topic_id] = {}
            for field in ont_header:
                if field == 'Key' or len(field)==0:
                    continue
                topics_dict[topic_id][field.strip()] = row[ont_header.index(field.strip())]
            topics_dict[topic_id]['encoded_text'] = topics_dict[topic_id]['Label'] + '. ' + topics_dict[topic_id]['Description']

        print('Serialising topics for ' + ont_label + '…',len(topics_dict))
        filename = model_path + ont_label + '_topics_dict.json'
        with open(filename, 'w') as f:
            json.dump(topics_dict, f)
            f.close()
        topic_encodings = encode_topics(ont_label,topics_dict,model_path,encoder,split_size=2)
        build_topic_segments_matrix(ont_label,topic_encodings,segment_encodings,model_path)
        print()

    filename = model_path + 'ontologies_dict.json'
    with open(filename, 'w') as f:
        json.dump(ontologies_dict, f)
        f.close()

    print('Finished processing ontologies')
